{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "goals",
   "metadata": {},
   "source": [
    "## Day 12: Feature Matching and the Ratio Test\n",
    "\n",
    "#### Goals\n",
    "* Know the major feature detector/descriptor algorithms available in OpenCV.\n",
    "* Understand how to match features between images using the SSD metric.\n",
    "* Understand the ratio test and why it's critical for finding good matches.\n",
    "* Be able to implement feature matching and apply the ratio test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate setup\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "### Recap: The Feature Pipeline\n",
    "\n",
    "Over the past few classes, we've been building up the pieces for feature-based computer vision:\n",
    "\n",
    "1. **Detection**: Find interesting keypoint locations (Harris corners)\n",
    "2. **Description**: Extract a feature vector at each keypoint (MOPS descriptor)\n",
    "3. **Matching**: Compare descriptors between images to find correspondences ← **Today's focus!**\n",
    "\n",
    "With matching, we'll finally be able to find the **same physical points** across different images - the foundation for panorama stitching, 3D reconstruction, tracking, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opencv_survey_header",
   "metadata": {},
   "source": [
    "## OpenCV Feature Detectors\n",
    "\n",
    "We already learned about the Harris Corners feature detector and the MOPS feature descriptor. These are \"student-level\" tech--a good introduction to the ideas, but not the most performant tech available. Before we start matching we'll survey the various feature detectors and descriptors you might want to use in OpenCV. \n",
    "\n",
    "Note that some of these detectors and descriptors come as joined pairs, and others can be mixed and matched.\n",
    "They have different tradeoffs between speed, accuracy, patent restrictions, and invariance properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpkit4nn9b",
   "metadata": {},
   "source": [
    "### Harris Corners (OpenCV Built-in)\n",
    "\n",
    "`cv2.cornerHarris()`\n",
    "\n",
    "**What it is:** The classic corner detector we've been implementing from scratch, available in OpenCV.\n",
    "\n",
    "**Key characteristics:**\n",
    "* Detects corners by analyzing eigenvalues of the \"structure tensor\"\n",
    "* Returns corner **locations only** (not a combined detector/descriptor like SIFT)\n",
    "* No scale invariance built-in\n",
    "* Can be combined with any descriptor (MOPS, SIFT descriptors, etc.)\n",
    "\n",
    "**Advantages:**\n",
    "* Fast corner detection\n",
    "* Well-understood classical algorithm\n",
    "* Good for tracking and simple scenarios\n",
    "* Can mix with any descriptor\n",
    "\n",
    "**Disadvantages:**\n",
    "* Only detects - doesn't describe (need separate descriptor)\n",
    "* Not scale invariant (corners \"repeat\" at different pyramid levels)\n",
    "* Less robust than modern integrated detector/descriptors\n",
    "\n",
    "**When to use:** Learning, simple tracking applications, or when you want to experiment with separating detection and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xz57xdqyv6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harris corner detection example\n",
    "img = cv2.imread(\"../data/yos1.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# cv2.cornerHarris - returns corner response map\n",
    "harris_response = cv2.cornerHarris(img, blockSize=5, ksize=3, k=0.04)\n",
    "\n",
    "# Threshold to find corner peaks\n",
    "threshold = 0.05 * harris_response.max()\n",
    "corner_mask = harris_response > threshold\n",
    "\n",
    "# Find corner coordinates\n",
    "corner_coords = np.argwhere(corner_mask)\n",
    "print(f\"cv2.cornerHarris found {len(corner_coords)} corners above threshold\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Show harris response map\n",
    "axes[0].imshow(harris_response, cmap='viridis')\n",
    "axes[0].set_title(\"Harris Corner Response Map\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show cornerHarris detections\n",
    "img_harris = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "for coord in corner_coords[::10]:  # Show every 10th corner to avoid clutter\n",
    "    cv2.circle(img_harris, (coord[1], coord[0]), 3, (255, 0, 0), -1)\n",
    "axes[1].imshow(img_harris)\n",
    "axes[1].set_title(f\"Detected Corners\\n(showing subset of {len(corner_coords)} total)\", fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sift_section",
   "metadata": {},
   "source": [
    "### SIFT (Scale-Invariant Feature Transform)\n",
    "\n",
    "**What it is:** The \"gold standard\" feature detector/descriptor developed by David Lowe in 1999.\n",
    "\n",
    "**Key characteristics:**\n",
    "* Uses Difference of Gaussians (DoG) for keypoint detection across scale space\n",
    "* Produces 128-dimensional descriptors\n",
    "* Highly robust to scale, rotation, and lighting changes\n",
    "* Selects a **characteristic scale** for each feature (unlike Harris, where each corner tends to be detected at many scales)\n",
    "\n",
    "**Advantages:**\n",
    "* Most distinctive and reliable feature matching\n",
    "* Excellent scale and rotation invariance\n",
    "* Patent expired in 2020 - now freely available!\n",
    "\n",
    "**Disadvantages:**\n",
    "* Not the fastest option\n",
    "* Higher memory usage (128-D floats)\n",
    "\n",
    "**When to use:** High-quality matching where accuracy matters more than speed (e.g., Structure from Motion, precise object recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sift_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT example\n",
    "img = cv2.imread(\"../data/yos1.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "print(f\"SIFT detected {len(keypoints)} keypoints\")\n",
    "print(f\"Descriptor shape: {descriptors.shape} (N × 128 float32)\")\n",
    "print(f\"First keypoint: location=({keypoints[0].pt[0]:.1f}, {keypoints[0].pt[1]:.1f}), size={keypoints[0].size:.1f}, angle={keypoints[0].angle:.1f}°\")\n",
    "\n",
    "# Visualize keypoints (size = scale, angle = orientation)\n",
    "img_with_kp = cv2.drawKeypoints(img, keypoints, None, \n",
    "                                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(img_with_kp, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"SIFT Features\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orb_section",
   "metadata": {},
   "source": [
    "### ORB (Oriented FAST and Rotated BRIEF)\n",
    "\n",
    "**What it is:** A fast, free alternative to SIFT, developed by OpenCV Labs in 2011.\n",
    "\n",
    "**Key characteristics:**\n",
    "* Combines FAST corner detector with BRIEF descriptor\n",
    "* Produces 256-bit binary descriptors (32 bytes)\n",
    "* Adds rotation invariance to BRIEF via orientation computation\n",
    "* Much faster than SIFT\n",
    "\n",
    "**Advantages:**\n",
    "* **Fast** - suitable for real-time applications\n",
    "* **Free** - no patent restrictions\n",
    "* Binary descriptors enable very fast matching (Hamming distance)\n",
    "* Low memory footprint\n",
    "\n",
    "**Disadvantages:**\n",
    "* Less accurate than SIFT\n",
    "* Not as scale-invariant as SIFT\n",
    "\n",
    "**When to use:** Real-time applications, mobile/embedded devices, or when you need free/open-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orb_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB example\n",
    "orb = cv2.ORB_create()\n",
    "keypoints_orb, descriptors_orb = orb.detectAndCompute(img, None)\n",
    "\n",
    "print(f\"ORB detected {len(keypoints_orb)} keypoints\")\n",
    "print(f\"Descriptor shape: {descriptors_orb.shape} (N × 32 uint8, representing 256 bits)\")\n",
    "print(f\"First descriptor (binary): {descriptors_orb[0]}\")\n",
    "\n",
    "img_with_orb = cv2.drawKeypoints(img, keypoints_orb, None, color=(0, 255, 0),\n",
    "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(img_with_orb, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"ORB Features ({len(keypoints_orb)} keypoints)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brisk_section",
   "metadata": {},
   "source": [
    "### BRISK (Binary Robust Invariant Scalable Keypoints)\n",
    "\n",
    "**What it is:** A binary feature detector/descriptor designed for efficiency (2011).\n",
    "\n",
    "**Key characteristics:**\n",
    "* Uses AGAST (faster variant of FAST) for detection\n",
    "* Binary descriptor based on sampling patterns\n",
    "* Scale-invariant via scale-space pyramid\n",
    "\n",
    "**Advantages:**\n",
    "* Very fast computation\n",
    "* Good rotation and scale invariance\n",
    "* Binary = fast matching with Hamming distance\n",
    "* Patent-free\n",
    "\n",
    "**Disadvantages:**\n",
    "* Less distinctive than SIFT\n",
    "* Similar performance to ORB in many cases\n",
    "\n",
    "**When to use:** Another good option for real-time applications, alternative to ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brisk_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRISK example (limited to 500 keypoints)\n",
    "brisk = cv2.BRISK_create()\n",
    "keypoints_brisk_all, descriptors_brisk_all = brisk.detectAndCompute(img, None)\n",
    "\n",
    "# Take only the top 500 keypoints by response strength\n",
    "keypoints_brisk_sorted = sorted(keypoints_brisk_all, key=lambda x: x.response, reverse=True)\n",
    "keypoints_brisk = keypoints_brisk_sorted[:500]\n",
    "descriptors_brisk = descriptors_brisk_all[:500]\n",
    "\n",
    "print(f\"BRISK detected {len(keypoints_brisk_all)} keypoints total\")\n",
    "print(f\"Showing top {len(keypoints_brisk)} by response strength\")\n",
    "print(f\"Descriptor shape: {descriptors_brisk.shape} (N × 64 uint8 binary)\")\n",
    "\n",
    "img_with_brisk = cv2.drawKeypoints(img, keypoints_brisk, None, color=(0, 165, 255),\n",
    "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(img_with_brisk, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"BRISK Features (showing top {len(keypoints_brisk)} of {len(keypoints_brisk_all)} keypoints)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_table",
   "metadata": {},
   "source": [
    "### Quick Comparison\n",
    "\n",
    "| Algorithm | Descriptor Type | Descriptor Size | Speed | Distinctiveness | Patent-Free | Best For |\n",
    "|-----------|----------------|-----------------|-------|-----------------|-------------|----------|\n",
    "| **SIFT** | Float | 128-D (512 bytes) | Slow | ★★★★★ | ✓ (since 2020) | High-quality matching, SfM |\n",
    "| **ORB** | Binary | 256 bits (32 bytes) | Fast | ★★★ | ✓ | Real-time, mobile apps |\n",
    "| **AKAZE** | Binary | 488 bits (61 bytes) | Medium | ★★★★ | ✓ | Good balance of speed/quality |\n",
    "| **BRISK** | Binary | 512 bits (64 bytes) | Fast | ★★★ | ✓ | Real-time alternative to ORB |\n",
    "\n",
    "**For this class:** We'll use SIFT for the rest of today's lesson since it produces the most reliable matches. In your projects, feel free to experiment with different algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matching_intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Matching\n",
    "\n",
    "Now we have two images, each with a set of feature descriptors. **How do we find which features correspond to the same physical point?**\n",
    "\n",
    "The basic idea: features that describe the same point should have **similar descriptors**.\n",
    "\n",
    "So we need:\n",
    "1. A way to measure how different two descriptors are\n",
    "2. An algorithm to find corresponding pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssd_intro",
   "metadata": {},
   "source": [
    "### SSD: Sum of Squared Differences\n",
    "\n",
    "The simplest distance metric for comparing two feature vectors $\\mathbf{f}$ and $\\mathbf{g}$ is the **Sum of Squared Differences (SSD)**:\n",
    "\n",
    "$$\n",
    "\\text{SSD}(\\mathbf{f}, \\mathbf{g}) = \\sum_{i=1}^d (f_i - g_i)^2\n",
    "$$\n",
    "\n",
    "where $d$ is the dimension of the descriptor.\n",
    "\n",
    "* **Small SSD** = descriptors are similar = likely a match\n",
    "* **Large SSD** = descriptors are different = not a match\n",
    "\n",
    "This is equivalent to squared Euclidean distance. We've seen this before in the Harris corner metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ssd_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd(f, g):\n",
    "    \"\"\"Compute sum of squared differences between two feature vectors.\"\"\"\n",
    "    return np.sum(np.square(f - g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toy_example",
   "metadata": {},
   "source": [
    "### Toy Example: Matching Two Sets of Features\n",
    "\n",
    "Let's work through a small example by hand (and with code). Suppose we have:\n",
    "* Image 1 with 4 features: `F1[:, i]` for i=0,1,2,3\n",
    "* Image 2 with 3 features: `F2[:, j]` for j=0,1,2\n",
    "\n",
    "Each feature is a 2-dimensional vector (normally they'd be 64-D or 128-D, but we'll keep it simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toy_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two sets of toy features (each column is one feature)\n",
    "F1 = np.array([\n",
    "    [0, 1, 4, 3],\n",
    "    [1, 0, 4, 1]], dtype=np.float32)\n",
    "\n",
    "F2 = np.array([\n",
    "    [2, 5, 1],\n",
    "    [1, 5, 2]], dtype=np.float32)\n",
    "\n",
    "print(\"F1 shape:\", F1.shape, \"(2 dimensions × 4 features)\")\n",
    "print(\"F2 shape:\", F2.shape, \"(2 dimensions × 3 features)\")\n",
    "print(\"\\nF1 features as columns:\")\n",
    "print(F1)\n",
    "print(\"\\nF2 features as columns:\")\n",
    "print(F2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distance_matrix_question",
   "metadata": {},
   "source": [
    "##### Discussion Question\n",
    "\n",
    "How would you find which feature in F2 is closest to each feature in F1?\n",
    "\n",
    "What data structure would help organize this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answer_space",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "distance_matrix_explanation",
   "metadata": {},
   "source": [
    "### Building a Distance Matrix\n",
    "\n",
    "The most straightforward approach: compute **all pairwise distances** in a matrix.\n",
    "\n",
    "Create a 4×3 matrix where entry $(i,j)$ contains `SSD(F1[:, i], F2[:, j])`.\n",
    "\n",
    "Then finding the closest match for each feature in F1 is just finding the minimum in each row!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_distances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all pairwise distances\n",
    "n1 = F1.shape[1]  # number of features in F1\n",
    "n2 = F2.shape[1]  # number of features in F2\n",
    "\n",
    "distances = np.zeros((n1, n2))\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        distances[i, j] = ssd(F1[:, i], F2[:, j])\n",
    "\n",
    "print(\"Distance matrix (F1 features × F2 features):\")\n",
    "print(distances)\n",
    "print(\"\\nRows = features in F1, Columns = features in F2\")\n",
    "print(\"Entry (i,j) = SSD between F1[:,i] and F2[:,j]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find_matches",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature in F1, find the closest feature in F2\n",
    "closest_matches = np.argmin(distances, axis=1)\n",
    "match_distances = np.min(distances, axis=1)\n",
    "\n",
    "print(\"Matches (F1 → F2):\")\n",
    "for i in range(n1):\n",
    "    print(f\"  F1[{i}] → F2[{closest_matches[i]}]  (SSD = {match_distances[i]:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brute_force_note",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "\n",
    "**Brute force matching:** Compare every feature in image 1 to every feature in image 2.\n",
    "\n",
    "* If each image has $n$ features with $d$-dimensional descriptors\n",
    "* We compute $n \\times n$ distances\n",
    "* Each distance computation costs $O(d)$\n",
    "* **Total:** $O(n^2 d)$\n",
    "\n",
    "For real images: $n \\sim 1000$ features, $d \\sim 128$ dimensions → millions of operations!\n",
    "\n",
    "**Optimizations:**\n",
    "* Vectorized distance computation: `scipy.spatial.distance.cdist` (or for binary: `cv2.BFMatcher` with Hamming distance)\n",
    "* Approximate nearest neighbors: kd-trees, FLANN, etc.\n",
    "* For today: we'll stick with brute force for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem_intro",
   "metadata": {},
   "source": [
    "### The Problem with Closest Match\n",
    "\n",
    "**Naive algorithm:** For each feature in image 1, take its closest match in image 2.\n",
    "\n",
    "**Problem:** What if the closest match isn't very close? \n",
    "\n",
    "Consider a repetitive texture like a fence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fence_image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fence image\n",
    "fence = cv2.imread(\"../data/fences.jpg\")\n",
    "fence_rgb = cv2.cvtColor(fence, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(fence_rgb)\n",
    "plt.title(\"The Fence Strikes Back: Why we can't just take the closest match\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fence_problem",
   "metadata": {},
   "source": [
    "In this image, many fence posts look nearly identical. A feature on one post will have:\n",
    "* Its **true match** on the corresponding post in the other image\n",
    "* Many **similar-looking features** on other posts\n",
    "\n",
    "If we just take the closest match, we might pick the wrong post! Even worse: the closest match distance won't tell us if we should trust the match.\n",
    "\n",
    "**Key insight:** Good matches should have a closest match that is **much closer** than the second-closest match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ratio_test_intro",
   "metadata": {},
   "source": [
    "## The Ratio Test\n",
    "\n",
    "**Lowe's Ratio Test** (from the SIFT paper): Don't just look at the closest match - look at the **ratio** between the closest and second-closest matches.\n",
    "\n",
    "For a feature $\\mathbf{f}$ in image 1, let:\n",
    "* $\\mathbf{g}_1$ = closest match in image 2 (smallest SSD)\n",
    "* $\\mathbf{g}_2$ = second-closest match in image 2 (second smallest SSD)\n",
    "\n",
    "Define the **ratio distance**:\n",
    "\n",
    "$$\n",
    "d_{\\text{ratio}} = \\frac{\\text{SSD}(\\mathbf{f}, \\mathbf{g}_1)}{\\text{SSD}(\\mathbf{f}, \\mathbf{g}_2)}\n",
    "$$\n",
    "\n",
    "**Accept the match** only if $d_{\\text{ratio}} < \\tau$ (typically $\\tau \\approx 0.8$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ratio_test_question",
   "metadata": {},
   "source": [
    "##### Discussion Questions\n",
    "\n",
    "1. What does $d_{\\text{ratio}} = 1.0$ mean?\n",
    "\n",
    "2. What does $d_{\\text{ratio}} = 0.5$ mean?\n",
    "\n",
    "3. Why is a **small** ratio distance a sign of a good match?\n",
    "\n",
    "4. What would happen on a feature from a repetitive fence post?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answer_space2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ratio_test_intuition",
   "metadata": {},
   "source": [
    "### Ratio Test Intuition\n",
    "\n",
    "**Good matches** (distinctive features):\n",
    "* Closest match is much closer than second-closest\n",
    "* Ratio distance is **small** (e.g., 0.5)\n",
    "* We're confident this is the right match!\n",
    "\n",
    "**Bad matches** (ambiguous/repetitive features):\n",
    "* Closest and second-closest are similar distances\n",
    "* Ratio distance is **close to 1.0**\n",
    "* Could match to multiple features - reject it!\n",
    "\n",
    "By rejecting ambiguous matches, we get fewer but **higher quality** correspondences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ratio_test_implementation",
   "metadata": {},
   "source": [
    "### Implementing the Ratio Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ratio_test_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our toy feature sets\n",
    "F1 = np.array([\n",
    "    [0, 1, 4, 3],\n",
    "    [1, 0, 4, 1]], dtype=np.float32)\n",
    "\n",
    "F2 = np.array([\n",
    "    [2, 5, 1],\n",
    "    [1, 5, 2]], dtype=np.float32)\n",
    "\n",
    "n1 = F1.shape[1]  # 4 features in F1\n",
    "n2 = F2.shape[1]  # 3 features in F2\n",
    "d  = F1.shape[0]  # feature dimensionality\n",
    "\n",
    "# Define SSD distance function\n",
    "def ssd(f, g):\n",
    "    return np.sum(np.square(f - g))\n",
    "\n",
    "# Compute all pairwise distances\n",
    "distances = np.zeros((n1, n2))\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        distances[i, j] = ssd(F1[:, i], F2[:, j])\n",
    "\n",
    "# Find closest matches for each feature in F1\n",
    "closest_matches = np.argmin(distances, axis=1)\n",
    "\n",
    "# Compute ratio distances\n",
    "ratio_distances = np.zeros(n1)\n",
    "for i in range(n1):\n",
    "    # Get sorted distances for feature i in F1\n",
    "    sorted_distances = np.sort(distances[i, :])\n",
    "    \n",
    "    # Ratio of closest to second-closest\n",
    "    d1 = sorted_distances[0]  # closest\n",
    "    d2 = sorted_distances[1]  # second-closest\n",
    "    \n",
    "    ratio_distances[i] = d1 / d2 if d2 > 0 else 0\n",
    "\n",
    "print(\"Ratio test results (F1 → F2):\")\n",
    "for i in range(n1):\n",
    "    print(f\"  F1[{i}] → F2[{closest_matches[i]}]  (SSD = {distances[i, closest_matches[i]]:.1f}, ratio = {ratio_distances[i]:.3f})\")\n",
    "\n",
    "# Apply ratio test threshold\n",
    "ratio_threshold = 0.3\n",
    "good_matches = ratio_distances < ratio_threshold\n",
    "\n",
    "print(f\"\\nMatches passing ratio test (threshold = {ratio_threshold}):\")\n",
    "for i in range(n1):\n",
    "    if good_matches[i]:\n",
    "        print(f\"  F1[{i}] → F2[{closest_matches[i]}] ✓\")\n",
    "    else:\n",
    "        print(f\"  F1[{i}] → (rejected) ✗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yosemite_intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Closing Demo: Matching Real Images\n",
    "\n",
    "Our plan:\n",
    "1. Load two overlapping images\n",
    "2. Detect SIFT features in both images\n",
    "3. Match features using OpenCV's built-in matcher\n",
    "4. Apply the ratio test\n",
    "5. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_yosemite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Yosemite images\n",
    "yos1 = cv2.imread(\"../data/yos1.jpg\")\n",
    "yos2 = cv2.imread(\"../data/yos2.jpg\")\n",
    "\n",
    "yos1_gray = cv2.cvtColor(yos1, cv2.COLOR_BGR2GRAY)\n",
    "yos2_gray = cv2.cvtColor(yos2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display both images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axes[0].imshow(cv2.cvtColor(yos1, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(\"Yosemite Image 1\", fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(yos2, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(\"Yosemite Image 2\", fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image 1 shape: {yos1.shape}\")\n",
    "print(f\"Image 2 shape: {yos2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect_yosemite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect SIFT features in both images\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "kp1, desc1 = sift.detectAndCompute(yos1_gray, None)\n",
    "kp2, desc2 = sift.detectAndCompute(yos2_gray, None)\n",
    "\n",
    "print(f\"Image 1: detected {len(kp1)} SIFT keypoints\")\n",
    "print(f\"Image 2: detected {len(kp2)} SIFT keypoints\")\n",
    "print(f\"Descriptor dimensions: {desc1.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "match_yosemite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features using brute-force matcher\n",
    "# We use knnMatch with k=2 to get the top 2 matches for ratio test\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "matches_knn = bf.knnMatch(desc1, desc2, k=2)\n",
    "\n",
    "print(f\"Found {len(matches_knn)} features with at least 2 matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ratio_test_yosemite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ratio test\n",
    "ratio_threshold = 0.75\n",
    "good_matches = []\n",
    "\n",
    "for match_pair in matches_knn:\n",
    "    if len(match_pair) == 2:  # Make sure we have 2 matches\n",
    "        m, n = match_pair  # m = closest, n = second-closest\n",
    "        # m.distance is the SSD to closest match\n",
    "        # n.distance is the SSD to second-closest match\n",
    "        if m.distance < ratio_threshold * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "print(f\"Total candidate matches: {len(matches_knn)}\")\n",
    "print(f\"Matches passing ratio test: {len(good_matches)}\")\n",
    "print(f\"Rejection rate: {100 * (1 - len(good_matches)/len(matches_knn)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_all_matches",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ALL matches (before ratio test)\n",
    "all_matches_img = cv2.drawMatches(\n",
    "    yos1, kp1, yos2, kp2, \n",
    "    [m[0] for m in matches_knn if len(m) == 2],  # take closest match from each pair\n",
    "    None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.imshow(cv2.cvtColor(all_matches_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"All Closest Matches (Before Ratio Test): {len(matches_knn)} matches\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_good_matches",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GOOD matches (after ratio test)\n",
    "good_matches_img = cv2.drawMatches(\n",
    "    yos1, kp1, yos2, kp2, good_matches, None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.imshow(cv2.cvtColor(good_matches_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Good Matches (After Ratio Test, threshold={ratio_threshold}): {len(good_matches)} matches\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze_matches",
   "metadata": {},
   "source": [
    "Compare the two visualizations above:\n",
    "\n",
    "**Before ratio test:** Many matches, but some are clearly wrong (crossing lines, misaligned features)\n",
    "\n",
    "**After ratio test:** Fewer matches, but they're (mostly) correct!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment_threshold",
   "metadata": {},
   "source": [
    "#### The Ratio Test Threshold\n",
    "\n",
    "(Try changing the `ratio_threshold` value in the cell below and re-run the matching visualization.)\n",
    "\n",
    "* **Lower threshold (e.g., 0.5):** Fewer but higher-confidence matches\n",
    "* **Higher threshold (e.g., 0.9):** More matches but lower quality\n",
    "\n",
    "Lowe's original SIFT paper recommended 0.8. For panorama stitching, 0.7-0.75 often works well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Today we completed the feature matching pipeline:\n",
    "\n",
    "1. **OpenCV Feature Detectors**: SIFT (best quality), ORB (fast/free), BRISK (fast/free)\n",
    "\n",
    "2. **Feature Matching**: Compare descriptors using distance metrics like SSD (sum of squared differences)\n",
    "\n",
    "3. **The Ratio Test**: Accept a match only if the closest match is significantly better than the second-closest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csse461",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
